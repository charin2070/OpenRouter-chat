# GoAI Timeline

**GoAI Timeline** is an AIOps-powered tool designed to streamline incident analysis by automatically processing and visualizing data from logs, chats, and monitoring systems.

The core problem it solves is the "information noise" generated by modern IT systems, which can make finding the root cause of a failure a time-consuming and manual process. GoAI Timeline automates this by:

1.  **Aggregating Data**: It collects and normalizes logs, chat messages, and alerts from various sources into a single view.
2.  **Detecting Anomalies**: Using machine learning, it identifies unusual patterns and pinpoints anomalous events that are likely related to the incident.
3.  **Visualizing Incidents**: It displays the correlated events on an interactive timeline, providing a clear, chronological view of the incident. This helps teams quickly understand the sequence of events and identify the root cause (RCA).

This approach helps reduce mean time to resolution (MTTR), improves the efficiency of support teams, and provides valuable insights for preventing future incidents.

For more details, you can read the full [Project Concept](./app/documentation/project-concept.md).


## Features

- ğŸ” **Google Authentication**: Secure sign-in with Google OAuth
- ğŸ¤– **Multi-AI Provider Support**: Choose between Google Gemma 3b and Mistral Medium models
- ğŸ›ï¸ **AI Provider Selection**: Easy switching between AI providers via sidebar dropdown
- ğŸ’¬ **Modern UI**: Clean, professional design with message bubbles and animations
- âš¡ **Real-time Streaming**: See AI responses appear in real-time as they're generated
- ğŸ“± **Responsive Design**: Works perfectly on desktop and mobile devices
- ğŸ›¡ï¸ **Error Handling**: Graceful error handling with retry functionality
- ğŸ§¹ **Chat Management**: Clear chat history with confirmation dialog
- ğŸ‘¤ **User Profile**: Display user information and sign-out functionality
- âŒ¨ï¸ **Keyboard Shortcuts**: Enter to send, Shift+Enter for new lines
- ğŸ¨ **Professional Styling**: Beautiful gradients, animations, and micro-interactions

## Technology Stack

- **Framework**: Next.js 13.5.1 with App Router
- **Language**: TypeScript with strict type checking
- **Styling**: Tailwind CSS with shadcn/ui components
- **Authentication**: NextAuth.js with Google OAuth
- **AI Integration**: OpenRouter API with multiple AI models (Google Gemma 3b, Mistral Medium)
- **Package Manager**: pnpm (recommended) or npm

## TOP-10 THINGS THAT YOU MIGHT KEEP IN MIND

0. **README.md - entry point for all dubug sessions** - only critical information about project context is stored here. Use it and keep up to date.
1. **99% actions user requests from QueryPanel component** - this component is the main component of the application and is responsible for the main functionality of the application. Always start debugging from this component.
2. **
## Getting Started

### Prerequisites

- Node.js 18+ 
- pnpm (recommended) or npm
- OpenRouter API key

### Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd gemma-chat-app
   ```

2. Install dependencies:
   ```bash
   pnpm install
   # or
   npm install
   ```

3. Set up environment variables:
   - Copy `.env.local` and update if needed
   - The OpenRouter API key is pre-configured for the Gemma 3b free tier

4. Start the development server:
   ```bash
   pnpm redev
   # or
   npm run redev
   ```

   The `redev` script includes a 20-second countdown and automatically opens your browser to `http://localhost:3000`.

### Alternative Development Commands

```bash
# Standard development server
pnpm dev
# or
npm run dev

# Build for production
pnpm build
# or  
npm run build

# Start production server
pnpm start
# or
npm start
```

## API Configuration

The application supports multiple AI providers through OpenRouter API:

### Available AI Models
- **Google Gemma 3b**: `google/gemma-2-9b-it:free` (Free tier)
- **Mistral Medium**: `mistralai/mistral-medium` (Premium tier)

### API Endpoint
- **Base URL**: OpenRouter API (`https://openrouter.ai/api/v1/chat/completions`)
- **Features**: Streaming responses, temperature control, token limits
- **Provider Selection**: Users can switch between AI providers via the sidebar dropdown

## Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ auth/[...nextauth]/ # NextAuth API routes
â”‚   â”‚   â””â”€â”€ chat/          # API route for OpenRouter communication
â”‚   â”œâ”€â”€ globals.css        # Global styles and Tailwind CSS
â”‚   â”œâ”€â”€ layout.tsx         # Root layout with metadata
â”‚   â””â”€â”€ page.tsx           # Main chat page
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ auth/              # Authentication components
â”‚   â”‚   â”œâ”€â”€ sign-in.tsx    # Google Sign-In component
â”‚   â”‚   â””â”€â”€ user-profile.tsx # User profile dropdown
â”‚   â”œâ”€â”€ chat/              # Chat-specific components
â”‚   â”‚   â”œâ”€â”€ chat-header.tsx     # Header with clear chat functionality
â”‚   â”‚   â”œâ”€â”€ sidebar.tsx         # Sidebar with AI provider selection
â”‚   â”‚   â”œâ”€â”€ message-list.tsx    # Scrollable message container
â”‚   â”‚   â”œâ”€â”€ message-bubble.tsx  # Individual message bubbles
â”‚   â”‚   â”œâ”€â”€ message-input.tsx   # Input field with send button
â”‚   â”‚   â”œâ”€â”€ error-message.tsx   # Error display with retry
â”‚   â”‚   â””â”€â”€ log-uploader.tsx    # Log file upload component
â”‚   â””â”€â”€ ui/                # shadcn/ui components
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ use-chat.ts        # Custom hook for chat functionality
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ auth-context.tsx   # Authentication context
â”‚   â”œâ”€â”€ types.ts           # TypeScript interfaces and AI provider configurations
â”‚   â”œâ”€â”€ openrouter.ts      # OpenRouter API integration with multi-provider support
â”‚   â”œâ”€â”€ mistral.ts         # Mistral API integration (alternative provider)
â”‚   â””â”€â”€ utils.ts           # Utility functions
â””â”€â”€ .env.local             # Environment variables
```

## Key Features Explained

### Multi-AI Provider Support
The application supports multiple AI providers, allowing users to choose the best model for their needs:
- **Google Gemma 3b**: Fast and efficient model for general conversation (Free tier)
- **Mistral Medium**: Advanced model with enhanced reasoning capabilities (Premium tier)
- **Easy Switching**: Users can switch between providers using the sidebar dropdown
- **Provider Persistence**: Selected provider is maintained during the session
- **Unified Interface**: All providers use the same chat interface and streaming responses

### Google Authentication
The application uses NextAuth.js with Google OAuth for secure user authentication:
- **Sign In**: Users can sign in with their Google account
- **Session Management**: Automatic session handling and persistence
- **User Profile**: Display user information and sign-out functionality
- **Protected Routes**: Chat interface is only accessible to authenticated users

### Streaming Responses
The application uses Server-Sent Events (SSE) to stream AI responses in real-time, providing a smooth chat experience.

### Message Status Indicators
Each message shows its status:
- **Sending**: Single check mark (gray)
- **Sent**: Double check mark (blue)  
- **Error**: Alert circle (red)

### Error Handling
- Network errors are caught and displayed with retry options
- API errors show user-friendly messages
- Graceful degradation when streaming fails

### Responsive Design
- Mobile-first design approach
- Adaptive message bubble sizes
- Touch-friendly interface elements
- Proper keyboard navigation

## Environment Variables

Create a `.env.local` file in the root directory with the following variables:

```env
# OpenRouter API Configuration (supports both Google Gemma and Mistral)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# AI Model Configuration (optional - defaults are used if not set)
AI_MISTRAL_MODEL=mistralai/mistral-medium
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=1000

# App Configuration
NEXT_PUBLIC_APP_NAME=Gemma Chat
NEXT_PUBLIC_APP_URL=http://localhost:3000

# Tetris Spinner Configuration
NEXT_PUBLIC_SPINNER_SPEED=300

# Google OAuth Configuration
# Get these from https://console.cloud.google.com/apis/credentials
GOOGLE_CLIENT_ID=your_google_client_id_here
GOOGLE_CLIENT_SECRET=your_google_client_secret_here

# NextAuth Configuration
NEXTAUTH_SECRET=your_nextauth_secret_here
NEXTAUTH_URL=http://localhost:3000
```

### Setting up Google OAuth

1. Go to the [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select an existing one
3. Enable the Google+ API (or Google Identity API)
4. Go to "Credentials" and create an "OAuth 2.0 Client ID"
5. Set the authorized redirect URI to: `http://localhost:3000/api/auth/callback/google`
6. Copy the Client ID and Client Secret to your `.env.local` file

### Generating NextAuth Secret

You can generate a secure secret using:
```bash
openssl rand -base64 32
```

### Environment Variables Explained

- **OPENROUTER_API_KEY**: Your OpenRouter API key for AI model access (supports both Google Gemma and Mistral)
- **AI_MISTRAL_MODEL**: Mistral model identifier (default: `mistralai/mistral-medium`)
- **AI_TEMPERATURE**: Controls response randomness (default: 0.7)
- **AI_MAX_TOKENS**: Maximum tokens in response (default: 1000)
- **NEXT_PUBLIC_APP_NAME**: The name of your application (displayed in UI)
- **NEXT_PUBLIC_APP_URL**: The base URL of your application
- **NEXT_PUBLIC_SPINNER_SPEED**: Speed of Tetris spinner animation in milliseconds (default: 300ms)
- **GOOGLE_CLIENT_ID**: Google OAuth client ID for authentication
- **GOOGLE_CLIENT_SECRET**: Google OAuth client secret for authentication
- **NEXTAUTH_SECRET**: Secret key for NextAuth.js session encryption
- **NEXTAUTH_URL**: The base URL for NextAuth.js callbacks

### Complete Setup Steps

1. **Install dependencies**:
   ```bash
   pnpm install
   ```

2. **Set up environment variables**:
   - Copy the example environment variables above
   - Create a `.env.local` file in the root directory
   - Add your Google OAuth credentials and NextAuth secret

3. **Start the development server**:
   ```bash
   pnpm dev
   ```

4. **Access the application**:
   - Open `http://localhost:3000`
   - You'll be redirected to the sign-in page
   - Click "Sign in with Google" to authenticate
   - After successful authentication, you'll be redirected to the chat interface

5. **Choose your AI provider**:
   - Use the dropdown in the sidebar to select between Google Gemma 3b and Mistral Medium
   - The selected provider will be used for all subsequent chat messages
   - You can switch providers at any time during your session

## Security Considerations

- **Authentication**: Secure OAuth 2.0 flow with Google
- **Session Management**: JWT tokens with secure storage
- **API Security**: API keys are stored server-side only
- **Rate Limiting**: Considerations implemented for API endpoints
- **Input Validation**: Validation on all API endpoints
- **Error Handling**: Secure error handling without exposing internals
- **Environment Variables**: Sensitive data stored in environment variables

## AI Provider Selection

The application supports multiple AI providers through a unified interface:

### How it Works
- **Provider Selection**: Users can choose between Google Gemma 3b and Mistral Medium via the sidebar dropdown
- **Session Persistence**: The selected provider is maintained throughout the user's session
- **API Routing**: All requests are routed through OpenRouter API with the appropriate model selection
- **Unified Experience**: Both providers offer the same streaming response experience

### Provider Comparison
- **Google Gemma 3b**: 
  - Free tier available
  - Fast response times
  - Good for general conversation and simple tasks
- **Mistral Medium**: 
  - Premium tier (requires credits)
  - Enhanced reasoning capabilities
  - Better for complex analysis and detailed responses

### Technical Implementation
- Provider state is managed in the `useChat` hook
- API requests include the selected provider parameter
- Backend validates and routes requests to the appropriate model
- All providers use the same streaming response format

## Performance Optimizations

- Efficient React rendering patterns
- Proper memoization of components
- Optimized bundle size with Next.js
- Streaming responses to reduce perceived latency

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add appropriate tests
5. Submit a pull request

## License

This project is licensed under the MIT License. See LICENSE file for details.

## Support

For issues and questions:
1. Check the [OpenRouter documentation](https://openrouter.ai/docs)
2. Review the GitHub issues
3. Create a new issue with detailed information

---

Built with â¤ï¸ using Next.js, TypeScript, and the power of Google's Gemma AI model.